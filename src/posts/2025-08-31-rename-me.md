---
layout: post
category: "Artificial Intelligence"
title: "rename me"
imagefeature:
description: 
draft: true
tags: ["Programming", "AI", "LLM"]
---
###

When I first heard about RAG (Retrieval Augmented Generation) in the context of large language models (LLMs), I was expecting it to be a complex and technical concept, even after doing a little bit of reading and discovering that the information generally was stored as embeddings in a vector database that there would be a fair amount of complexity involved.  But eventually I realized that RAG is really a very straightforward concept, you add (or augment) the prompt to the LLM with information that is relevant to the question being asked.  

It is as simple as supplying the prompt with the information required to answer the question and then using the power of the LLM to generate a human response that isn't just a regurgitation of the supplied information, but a synthesis of the information that is relevant to the question being asked.  

For the past 7 blog posts we have been using the recipes from the [Recipe Folder](https://web.archive.org/web/20220630230600/http://recipe-folder.com/) database to experiment with various aspects of Machine Learning.  We are going to do the same here, but this time use an LLM to help create a recipe suggestion system.  The idea is that the user will provide a list of ingredients they have on hand and the LLM will suggest a recipe that can be made with those ingredients.  The LLM will use RAG to augment the prompt with the recipes from the Recipe Folder database based on the ingredients provided by the user.

Lets use a Jupyter notebook to explore this idea.  First lets switch to the rag branch of the repository.

```bash
git checkout rag
```

and install the requirements for this blog post:

```bash
./.venv/Scripts/activate # Windows
# or
source ./.venv/bin/activate # Linux/MacOS
pip install -r requirements.txt
```

Now we can look at the notebook that we will be using to explore this idea.  Open the `rag.ipynb` file in Jupyter notebook.  You have to run the first cell, but it just imports the required libraries and sets up some paths.

```python
# Load a small pre-trained embedding model (open local model)
model = SentenceTransformer('all-MiniLM-L6-v2')  # open-source embedding model
```

First, we load the SentenceTransformer model that we will be using to create the embeddings for the recipes.   The model is downloaded from the internet the first time it is run, so the first time it is loaded, it will take a little time.

```python
# Ensure the labels directory exists
if not labels_dir.exists():
    print(f"Directory {labels_dir} does not exist. Please create it and add JSON files.")
    exit(1)
```    

Next, we load the JSON files that contain the recipes.  We will use the `SentenceTransformer` model to create embeddings for each recipe.  We will store the embeddings in a database that we will use to create the recipe suggestion system.   We use the ipwidgets library to display a progress bar while we are loading the recipes and creating the embeddings. 

```python
# Collect JSON files
files = sorted([p for p in labels_dir.iterdir() if p.is_file() and p.suffix == '.json'])

progress = widgets.IntProgress(value=0, min=0, max=len(files), description='Progress:')
pct = widgets.HTML(value="0%")
display(widgets.HBox([progress, pct]))
embedding_db = []  # list to store {"id", "name", "vector"} for each recipe
for idx, fp in enumerate(files):
    try:
        text = fp.read_text(encoding='utf-8')
        obj = json.loads(text)   
    except Exception as e:
        # Log and continue on parse/read errors
        print(f'Skipping {fp.name}: {e}')
        continue

    progress.value = idx + 1
    pct.value = f"{int(100 * (idx + 1) / len(files))}%"

    # add source filename so downstream steps know which file produced this record
    if isinstance(obj, dict) and obj.get("title") and obj.get("ingredients"):
        title_instructions_file = {
            
        }
        text = f"{obj["title"]}: " + ", ".join(obj["ingredients"])
        vector = model.encode(text)            # get embedding vector (e.g., numpy array)
        embedding_db.append({
            "title" : obj["title"],
            "ingredients": obj["ingredients"],
            "source_file": fp.name,
            "vector": vector
        })
        
progress.value = len(files)
pct.value = "100%"

pickle.dump(embedding_db, open(model_path, 'wb'))
print(f"Stored {len(embedding_db)} recipe embeddings in the database.")
```

SentenceTransformer is a powerful library that can be used to create embeddings for text.  It is a wrapper around the Hugging Face Transformers library and the PyTorch library.  It is straightforward to use and provides a variety of pre-trained models (encoders) that can be used to create embeddings.  In the most basic terms it converts the text of a string (in our case the title and the ingredients of the recipe) into a vector that can be used to compare the similarity of two strings.  Depending upon which model you select, you will get different features of the sentence encoded, with MiniLM being a good balance of features and size.  As it has a fairly small number of parameters and gets its performance and small size from [attention distillation](https://sh-tsang.medium.com/brief-review-minilm-deep-self-attention-distillation-for-task-agnostic-compression-of-0be4516d6922).  The particular MiniLM we chose (all-MiniLM-L6-v2) is a 6-layer Transformer, with 33 million parameters and an embedding dimension of 384.  Given that a whole book could probably be written about various Transformer models and the MiniLM's [advancements in particular](https://arxiv.org/abs/2002.10957) we will leave it to the fact that the SentenceTransformer is powerful tool to extract semantic meaning from text that can be stored in an embedding database (here we use an in-memory database but in future posts we will a Vector database like [Chroma](https://github.com/chroma-core/chroma) or [Faiss](https://github.com/facebookresearch/faiss) to store the embeddings.

The SentenceTransformer is much more complex than the TF-IDF (TfidVectorizer) we used in the unsupervised learning blog post.  When we encoded the ingredients with a TF-IDF we lost any semantic meaning of the ingredients, we just got a sparse vector with one dimension per vocabulary word.  In the SentenceTransformer we get a dense vector with a fixed number of embeddings (384 for the MiniLM) which captures the semantics of the words because it is using a pre-trained transformer model.  While we could have a used a TF-IDF vectorizer for this purpose, we chose to use the SentenceTransformer because it is much more powerful and is what you would use for most real-world RAG applications (ingredients don't have nearly as much semantic meaning as say a company Knowledge Base or Product Faq but we might gain some deeper understanding of the ingredients from the recipe from using the SentenceTransformer).  And while there are other Transformer Models (BERT, RoBERTa, XLNet, etc.) that can be used for RAG, we chose to use the MiniLM because it is a small model that is generally the de-facto standard for RAG.

In the next cell in the notebook, we test extracting recipes from a list of ingredients.   We use the `find_best_matches` function to find the top 10 recipes that match the ingredients provided by the user. 

```python
def find_best_matches(query_ingredients, top_n: int = 10):
    """
    Return the top_n matching recipe entries from embedding_db for the given
    query_ingredients list. Returns a list of tuples: (entry_dict, score).
    """
    # Embed the query ingredients list into a vector
    query_text = ", ".join(query_ingredients)
    q_vec = model.encode(query_text)
    
    # Stack all stored vectors into a matrix (N x D)
    mat = np.vstack([entry["vector"] for entry in embedding_db])
    
    # Compute cosine similarities in a vectorized way
    q_norm = np.linalg.norm(q_vec)
    mat_norms = np.linalg.norm(mat, axis=1)
    denom = mat_norms * (q_norm if q_norm != 0 else 1e-12)

    # avoid division by zero
    denom[denom == 0] = 1e-12
    sims = (mat @ q_vec) / denom

    # Get top_n indices (highest similarity)
    k = min(int(top_n), sims.size)
    top_idx = np.argsort(sims)[::-1][:k]

    # Build results: (entry, score)
    return [(embedding_db[i], float(sims[i])) for i in top_idx]
    
# Example query
user_ingredients = ["cheese", "bread", "mustard", "pickle"]  
matches = find_best_matches(user_ingredients)
for entry, score in matches:
    print(f"{entry['title']} [{entry['source_file']}] — {score * 100:.2f}%")
```

In the `_find_best_matches` function, we first generate the encoding for the ingredient query.  Then we Compute the L2 (Euclidean) norm of the query vector. This returns a scalar (shape () -- `q_norm = np.linalg.norm(q_vec)`).  Next we compute the L2 norm for each row of mat (each stored embedding -- `mat_norms = np.linalg.norm(mat, axis=1)`). The result is a 1‑D array of length N (one norm per stored vector).  Then we build the denominator for cosine similarity (`denom = mat_norms * (q_norm if q_norm != 0 else 1e-12)`): elementwise product of each stored-vector norm and the query norm. If the query norm is zero we substitute a tiny epsilon (1e-12) to avoid division by zero.

This prepares the denominators so you can compute cosine similarity in a vectorized way as: `sims = (mat @ q_vec) / denom` where `mat @ q_vec` produces the dot product between the query and each stored vector (shape (N,)), and dividing by denom yields the cosine similarity for each stored vector.  We are computing [cosine similarity](/posts/2025-08-30-serving-the-cookbook-creating-an-endpoint-for-recipe-recommendations#startup-script) (which we discussed in the previous [blog post](/posts/2025-08-30-serving-the-cookbook-creating-an-endpoint-for-recipe-recommendations#startup-script)) to find recipes that have ingredients contained in the query.  The top 10 results are returned in the `matches` variable, and we just output them in the notebook.

Now we can use those results, to augment our prompt with the recipes from the Recipe Folder database. 

```python
import requests
import os

useOpenAI = True

user_ingredients = ["carrots", "edamame", "corn", "pork"]  # user has these ingredients

# Suppose we take the top 3 matches from the embedding search (for demonstration)
# Here we'll use the embedding search result; in practice, you would sort and pick top N.
top_matches = find_best_matches(user_ingredients, top_n=25)

# Construct a prompt with the query and top matches for GPT-4.1-nano
candidate_list_str = ""
for i, (recipe, score) in enumerate(top_matches, start=1):
    ingr_list = ", ".join(recipe["ingredients"])
    candidate_list_str += f'{{ "recipe_name": "{recipe['title']}", "file_name": "{recipe['source_file']}", "ingredients":  "{ingr_list}" }}'

recipe_style = "recipe that is good for lunch"

system_msg = (
    "You are a helpful cooking assistant. A user has certain ingredients, and we have some candidate recipes from a database. "
    "Choose which recipe is the best match for the user's ingredients and give a reason for choosing it."
)
user_msg = (
    f"The user has the following ingredients: {', '.join(user_ingredients)}.\n"
    f"The candidate recipes are:\n[{candidate_list_str}]\n"
    f"The user wants a recipe that matches ``{recipe_style}``\n"
    "Which recipe from the candidate recipes best matches the user's ingredients and has the style that user wants?" 
    "Respond with the recipe name and file_name and the reason for picking this recipe in JSON. Like this: "
    "{\"recipe_name\": \"Tomato Soup\", \"file_name\": \"recipe_00031.json\", \"reason\": \"Because soup is good food\"}"
)

if (useOpenAI and os.getenv("OPENAI_API_KEY")):
    API_KEY = os.getenv("OPENAI_API_KEY") # load from env
    api_url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    model_name = "gpt-4.1-nano" # Cheapest fast model currently from OpenAI (5.1-nano takes 5 times longer)
else:
    # Use Ollama local model server
    api_url = "http://127.0.0.1:11434/v1/chat/completions"
    headers = {
        "Content-Type": "application/json"
    }
    model_name = "gemma3:1b"

data = {
    "model": model_name,
    "messages": [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg}
    ]
}

if (not model_name.startswith("gpt-5") ):
    data["temperature"] = 0.3

# Send the HTTP request to OpenAI
response = requests.post(api_url, headers=headers, json=data)
response_json = response.json()

if (response_json.get("error") or response.status_code != 200):
    print(f"Error: {response.status_code} - {response.text}")
else:
    # Extract the assistant's answer (recipe title and id)
    best_recipe_answer = response_json["choices"][0]["message"]["content"].strip()
    print(f"{model_name}'s answer: {best_recipe_answer}")
```




