---
layout: post
category: Programming
title: "I built an Interactive CV"
imagefeature:
description: "A weekend build: a data-driven resume with a digital assistant, streaming Q&A, and a job-fit analyzer."
draft: true
tags: ["Programming", "LLM", "AI", "Career"]
date: 2026-01-25
---

## A quick weekend build

I took a short break from the larger project I am working on and spent a weekend knocking out an idea I had been sitting on for a few months, creating a chatbot for this blog. Most of my posts start at five to ten times the final length, and I have kept those drafts for years. It felt like perfect RAG fodder, especially now that I have turned off comments because I did not have time to clean up the spam.

That idea connected to something else I had been thinking about, a better way to expose my more complete resume. When I started applying for jobs this time around I had a four page resume, and everyone told me to get it down to one page or two at most. A resume chatbot could let people explore the supplemental detail only if they cared.

Then I saw Nate Jones’ “LinkedIn is Dead” video and realized my weird idea might not be so weird after all. So I went to work building an [Interactive CV](https://agingcoder.com/cv/), borrowing his “Fit Assessment” idea but building from scratch (with the help of AI coding assistants) and not building it on a no-code site like [Lovable](https://lovable.dev) or [Replit](https://replit.com).

The initial framing was built using [OpenCode](https://opencode.ai) using the free [Big Pickle model](https://www.crackedaiengineering.com/ai-models/opencode-big-pickle) (which is apparently just [GLM 4.6](https://github.com/anomalyco/opencode/issues/4276)), but quickly switched over use gpt-5.2-codex which it could access through my ChatGPT pro account.  I got it form a plan, based on some initial prompts and from the fact that I had decided I wanted to host the actual service on CloudFlare to play around with CloudFlare workers -- I had recently started having CloudFlare do the caching on my Blog
 I spent about five hours building the code. It would have been faster if I had tokens left for Claude Code, but I also wanted to try doing something a bit more real in OpenCode.

## What it does

On the surface it is a normal CV page with summary, experience, skills, projects, and education, but each section has a More info hook that opens a focused chat. The top of the page also has three primary actions, Digital Assistant, Fit Assessment, and About this Interactive CV, which act like entry points into the conversational layer.

The Digital Assistant is a chat UI with sample questions and streaming answers. The Fit Assessment lets you paste a job description or provide a public URL and then returns a structured assessment. About this Interactive CV opens a short behind the scenes explainer and prompt details.

Most of the work was not the chat itself, it was making the UI feel like a real product. The little details mattered, including streaming responses so the assistant starts typing immediately, a typing indicator, a scroll cue when the chat is long, and the conversational More info buttons that tie each resume section to curated prompts.

Some of the weekend grind tasks were the unglamorous ones. I had to get the chat to feel like a proper chat with typing, stream responses so answers start immediately, convert my earlier Python RAG engine into JavaScript, research Cloudflare embeddings and the limits of precomputing vectors locally, move the chat UI from JavaScript to TypeScript (OpenCode struggled here, so I used Codex), and clean up AI generated 1,000 line functions while tightening prompts to reduce hallucinations.

## How it is built

On the front end, the CV is data-driven and rendered by 11ty using `src/_data/cv.json` and the `src/cv.njk` template. The page is static, but most behavior comes from a small TypeScript app under `cv/` that Vite bundles into `cv.js`, which the template loads. That script sets up the chat modal, the scroll detection that shows the floating chat button after you scroll, and the toggles that expand experience and project entries without turning the whole CV into a giant wall of text.

The chat UI is intentionally lightweight but carefully tuned. It uses `streaming-markdown` so answers can stream and still render markdown incrementally without jarring reflows. I wrote a slow renderer that appends text in small chunks to create the illusion of typing, and I added a typing indicator and a scroll cue that says "More below" when the conversation grows longer than the visible area. Links generated by the assistant are forced to open in a new tab for safety and to keep the CV page from losing context.

The chat keeps conversation state in `sessionStorage` with a 24 hour expiry so the thread survives page refreshes without pretending to be permanent. Before a new message is sent, I dedupe repeated user assistant pairs so the back end sees a cleaner history instead of a pile of retries. There is also a persistent disclaimer at the top of the chat window that warns people the responses are generated by an LLM, grounded in over 20,000 words of background data, and may still be wrong.

The backend API lives in a Cloudflare Worker under `api-worker/` and is built with Hono. I wanted a small, fast surface area, so there are two primary endpoints. The first is `/api/chat`, which accepts a message or a message history, injects CV context plus RAG context, and streams tokens back to the browser. It uses OpenRouter behind the scenes, defaults to `gpt-4o-mini`, and restreams the upstream response into a plain text stream that the front end can render smoothly. There is a daily rate limit enforced via KV, with the limit and allowed CORS origins configured by environment variables, and optional analytics logging for questions, answers, and exact match hits.

The second endpoint is `/api/fit-assessment`, which takes either pasted text or a job posting URL. If a URL is provided, the worker fetches the page and does basic HTML stripping to extract readable text, but it cannot execute JavaScript, so it will fail on many client rendered job boards. The response is a strict JSON object that includes a verdict of strong, moderate, or weak, along with matches, gaps, and a short recommendation. It also includes a `jobPostingJudgment` field that lets the model say this is not a job posting when someone pastes the wrong thing.

For RAG, I keep about 23,000 words of Q and A context in `api-worker/src/rag-data/questions.json` plus a structured CV record. I spent more than double the build time writing and curating that backing material, because the bot is only as good as the facts it can retrieve. The worker generates embeddings using Cloudflare's `@cf/baai/bge-small-en-v1.5` model, chunks longer answers into smaller windows, caches embeddings in KV, and then ranks the best matches with cosine similarity. It also does exact question matching by hashing normalized questions so I can return a verbatim answer for a few sensitive cases instead of sending them through the model. If embeddings fail for any reason, there is a deterministic fallback so the system still works in a degraded mode instead of failing outright.

## Want to try it?

The Interactive CV is live at https://agingcoder.com/cv/ and the source code is open on GitHub at https://github.com/kriserickson/aging-coder.

## Still alpha

At the time of writing I still consider this alpha code quality. There are only a couple of tests, the worker is still JavaScript, and I have not written any end to end tests yet. There is not enough logging or error handling, and I did not run the build through any AI code review tools. I also need proper model evaluation and better analytics so I can track answer quality over time.

But I still consider knocking out a project like this in about five hours a huge success in showing what AI assisted coding can do. It is a small experiment, but it turns a static resume into something more interactive, more honest, and hopefully more useful.


