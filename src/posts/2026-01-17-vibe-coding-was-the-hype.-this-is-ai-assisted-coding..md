---
layout: post
category: Programming 
title: "Vibe Coding Was the Hype. This Is AI-Assisted Coding."
imagefeature:
description: Six more months. Fewer illusions. Better tools. Same hard problems. 
draft: true
tags: ["Programming", "LLM", "AI"]
---

## Introduction

Six months ago, I wrote about [LLMs and coding](/posts/llms-and-coding-6-months-later/) with cautious optimism, I hadn't put my finger on exactly what to use AI assistance for but in my mind it was great for throwaway prototypes, busy work (like documenting APIs), building test scaffolds (though I wasn't fully trusting it to write valid tests) and doing things like writing a helper function where previously you would pull in a library. A couple of weeks ago, I reviewed *[Vibe Coding](/posts/book-review-vibe-coding/)* and found myself nodding along with parts of it while wondering if there was any meat on the bone and whether or not it was wildly out-of-date.  I also argued that the book wasn't really arguing for `Vibe Coding` in the [Karpathy](https://en.wikipedia.org/wiki/Andrej_Karpathy) definition which you don't actually look at the code, but putting [DevOps](https://en.wikipedia.org/wiki/DevOps) and [Software Engineer Practices](https://itrevolution.com/product/the-phoenix-project/) around AI assisted coding -- which I agreed with.   Over the past few weeks, I decided that I had to let go of my need to live in the IDE and experiment with the command-line tools that have become the rage over the past few weeks - not only use the latest models (which I had been doing in [Cursor](https://www.cursor.com/), VS Code and the various JetBrains IDEs) but embrace the new command line tooling like [Claude Code](https://claude.com/product/claude-code), [OpenCode](https://opencode.ai) and [Codex CLI](https://developers.openai.com/codex/cli).  

I should point out that I’m revisiting this topic from a very different place: unemployed, budget-constrained, and forced to live squarely inside the twenty-dollar-a-month tier of modern AI tooling. That constraint turns out to be clarifying in ways that are hard to appreciate when cost isn’t a concern. When you don't have a an unlimited budget for tokens things are a little different from the worlds that people like [Steve Yegge](https://steve-yegge.medium.com/gas-town-emergency-user-manual-cf0e4556d74b) are living in with his desire to run tens of CLI agents at the same time -- I can't even comment on that nor do I want to live in a world where I don't care what the code looks like.  

This post is some thoughts on building a new project (which I will detail in a future post) with CLI's on a limited budget.  It isn't Vibe Coding: I am looking at every diff, and reviewing every pull request (after AI review).  I have tests (both unit and e2e) around everything, and though they were mostly written by AI, I instructed the AI what to test, and I review the test code much more closely than I review everything else.  But in this project, AI has written over 90% of the code.

## The New Project

Right now, my tooling setup is pretty complicated because I not only want to try out different tools, but I am being forced to switch between tools as tokens run out. I’m using Claude Code (Claude Sonnet 4.5 for most day-to-day coding work, leaning on Opus 4.5 only when I’m dealing with genuinely complex reasoning or planning problems that deserve slower thinking). Github Copilot Agent mode for small tasks, lint fixes, typescript fixes, small errors and fixing breaking tests.  I also am using ChatGPT Codex, but more as a second opinion than a primary driver when Claude Code runs out of tokens. Everything lives in GitHub, and I am using Copilot to do the code review on pull requests.  And recently I have started playing with OpenCode with the free models.  I have have assigned issues to CoPilot through the issues in GitHub, I have had the cloud version of Claude make pull requests I have made on my phone.  I have had a lot of fun (and a lot of frustration), and I feel that the programming world has definately changed forever.

What surprised me most over the last few months is just how capable AI Coding agents have become. Not in a demo-friendly way, and not in a “look what it can do on a greenfield example” sense, but in the unglamorous daily grind of real development. It holds context better, makes fewer wild assumptions, and produces code that usually compiles, runs, and fits into an existing system without too much coercion (though lint and a lot of tests are definately necssary).

A lot of my current workflow is flipping between the various AI tools because I find myself constantly running out tokens, whether it is for a few hours (with Claude code running of Current Session tokens when I ask it to perform a relatively complex task) or days or weeks (its only the 17th of the month today and I have used 92% of my premium tokens with Github CoPilot).

## The Previous Project

Six months ago, it was absolutely possible to “vibe code” a small app like [Paddle-Roster](https://paddle-roster.com). 

<img style="border: 1px solid #000; margin: 10px; width: 60%" src="/img/vibe-coding/paddle-roster-screenshot.webp">

Paddle-Roster is a simple Pickle Ball (or badminton, or any sport that requires 4 players 2 on each side) [scramble](https://pickleballspots.com/what-is-a-pickleball-scramble/) matching app - you can see the source code on [GitHub](https://github.com/kriserickson/paddle-roster).  I used agents in Copilot to build the initial project (though I did use the vite-cli to build the original scaffold). The scaffolding came together quickly. The UI was serviceable. The problem showed up where problems always show up: in the core logic. At the heart of that app is a match-generation algorithm that has to balance skill levels, bye's, different opponents, different partners, not playing on the same court, and other various edge cases. On the surface, it sounds straightforward. In practice, it’s anything but. The first version generated by an LLM worked well enough to be convincing, which turned out to be part of the problem. As soon as real usage began, subtle issues surfaced. Certain players were favored. Some combinations repeated too often. Others were silently excluded. Performance degraded under specific conditions (the complexity of one of the algorithm I tried was n<sup>n</sup>).

Over time, that algorithm has gone through four separate incarnations. I manually rewrote it. I collaborated with Opus 4.5 on alternative approaches. I iterated on versions that looked better on paper but failed under real data. Even now, it isn’t perfect.

In my mind AI assisted coding was great, especially for small GreenField projects that could be almost one-shot (generating a fully functional application or complex feature using a single natural language prompt).  If you had a project that you wanted for yourself, and didn't need to worry about the complexities of security, authentication, edge-cases, and the like, 6 months ago agents in a VS Code/Cursor/Windsurf could easily create most of what you needed.  

## What is Different

So what has changed.  The models are better, a lot better.  I don't know how much they have improved for non-coding, as I felt that they have been pretty good for non-coding things since 4o.  I used the ChatGPT app a ton on my last vacation to France for everything from planning a 2 hour blitz of the Louvre, to reading Latin inscriptions on random buildings, to seeing if it could read Egyptian hieroglyphs (interestingly it admitted that it can't translate hieroglyphs but it could tell me the text of the particular stella I had been looking at since it was clearly in its corpus), to the history of the Cathars.   For low stakes information (if it gets the name of a Cathar King wrong, I will have forgotten the name within the day anyway) ChatGPT, mostly because of having a pretty good mobile app, has been super useful for over a year.  I still keep a document of some of the surprisingly delicious recipes it has come up with when I give it the contents of my fridge/freezer/pantry and tell it I am in the mood from something with a Spanish/Middle-Eastern/Mexican/Indian/French feel - and that document stretches back a couple of years and dozens of dishes.  

But something has changed with both the CLI tooling and the models in the past few months, and it's not just Claude Opus 4.5 like the rest of the world would have you believe.  All of the tools are feeling more competent.  For example, because I am so token starved (I can get one or two features done in a session before Claude Caude runs out of tokens and gives me some random period to wait) I pass off the task of finalzing a task to Copilot in Agent mode.  Things like getting the type-checks to work in TypeScript (`tsc --noEmit`), fixing lint issues (I have switched to biome for speed and a sane config file), or getting the test to work properly with the Free Models in Copilot (currently I am using Raptor Mini because I have found myself at 90% usage of premium models by the middle of the month) and 90% of the time it can correctly fix the problem.  It seemed like even a month ago or so, with this same workflow I would get to the "Copilot has been working on this problem for a while, do you want to continue" almost half the time and had to do the dance of switching betwen free models (I had been switching between [Raptor Mini](https://dev.to/vevarunsharma/so-what-is-github-copilots-raptor-miniand-why-should-devs-care-3n30), Grok Code Fast 1 and GPT 4o) and sometimes just had to manually fix the problems.  I also found that a month or so ago CoPilot agent would regularly neuter the tests rather than fix them, create horrible types rather than fix the typescript properly and use lint ignores rather than fixing the problem, now those occurances are much less frequent - and I am not sure that models have changed that much -- their names certainly haven't.

I don't have any metrics to quantify this, things just feel better (I guess that is very appropriate for Vibe coding).  I kind of wish I had kept better track of how often I felt the previous tools where failing or working against me and how often it is happening now.  I mean I still have to do multiple prompts frequently to get it do what I want, and it still misunderstands what I ask it to do a few times a day but, like I said, it really just "feels" better.

## A Quick Example

I took a short break from the larger project I am working on (like I said, to be revealed later), spent the weekend knocking out an idea I had a few months ago about creating a ChatBot for this blog.  Whenever I write a blog post, it initially starts at about 5-10x the length of my ramblings and thoughts, just trying to get out every thought I had about the topic and it gets winowed down to something (hopefully) readable over time and through editing.  For the past couple of years I have kept these and thought it might be interesting to make them available (not to the readers, good lord I wouldn't want to punish you with that) as data to send as Rag and let people "Chat" with the post so that they can have a more interactive experience with the blog (especially now that I have turned off comments because I didn't have time to clean the 90% spam they produced).  Anyways, this had been percolating in my brain, and then I thought that it might be a much better way to expose people to my more complete resume (when I first started applying for Jobs this time I had a 4 page resume and was quickly told by everyone that you had to get it down to 1 page or at most 2), if they were interested by adding a Chatbot to the resume.  I could provide it with a lot of supplimental information which people could get from Chatting with it.  Then a few days ago [Nate Jones](https://natesnewsletter.substack.com) video on [LinkedIn is Dead](https://www.youtube.com/watch?v=0teZqotpqT8&t=9s) came accross my YouTube feed, and I realized that my wierd idea may not be so wierd after all.  And so I went to work building an [Interactive CV](https://agingcoder.com/cv/), borrowing his idea of a Fit Assesment, but building it with AI Coding assistants.   The initial build coming from [OpenCode](https://opencode.ai) using the free [Big Pickle model](https://www.crackedaiengineering.com/ai-models/opencode-big-pickle)(which is apparently just [glm 4.6](https://github.com/anomalyco/opencode/issues/4276)).  

I spent about 5 hours building the code (it would have been a lot faster if I had tokens left for ClaudeCode but also I really wanted to try doing something a little more real in OpenCode), most of which was the iterative process of making the page look and feel right.   

    - Getting the Chat to feel like a proper Chat with typing
    - Streaming the results from the AI model so that it started "typeing" as quick as possible
    - Converting the python [Rag Engine][https://agingcoder.com/posts/rag-time-cooking-up-smart-recipe-suggestions/] I had written earlier, into JavaScript.
    - Research into the various [Cloudflare AI Embeddings](https://developers.cloudflare.com/workers-ai/models/bge-small-en-v1.5/) and whether or not I could precalculate them locally with Hugging Face (it turns out you can't, the embeddings end up with different values which I discovered from another quick experiment that I had OpenCode write).  
    - Also as the chat interface started in JavaScript it quickly became way to large for my liking to have it non-typed so I got OpenCode to move it to typescript, which kind of failed so I had to use Codex to get that working.  
    - I also got it to clean up the code a lot -- AI's still love to generate 1000 line functions which just will not stand.  
    - Going back and forth with various ChatBots on the System and User prompts, trying to get them to be as accurate as possible without hallucinating.
  
  At the time of writing this still consider the project to be alpha code quality (there is currently only a couple of tests, the Cloudflare worker is still JavaScript, and there isn't enough logging or error handling).  I still haven't written any e2e tests yet, which is the next thing to do.  I need to write some model evaluation code, as I just played with a few models (gpt-5-mini is better but it is pretty slow, gpt-4.1-mini is pretty good but I found that 4o-mini was almost as good 4.1 mini and less than half the cost) and should probably do the work to properly evaluate the models other than in the feels.  In a real project I would also add a lot more logging and analytics, not only to keep an eye on AI usage but to see how the AI is actually performing.  I also didn't do this with pull requests like I usually do, and so none of the code has been reviewed by a AI Code Review tool (like [CoPilot](https://docs.github.com/en/copilot/concepts/agents/code-review), [CodeRabbit](https://www.coderabbit.ai), [Greptile](https://www.greptile.com), or Cursors [BugBot](https://cursor.com/bugbot))

  But I consider knocking out a project like this in 5 hours to be a huge success in showing what AI Assisted coding can do.  I spent more than double the time working on the [backing material](https://github.com/kriserickson/aging-coder/blob/main/api-worker/src/rag-data/questions.json)(which is almost 20,000 words of background text) to feed the RAG engine.  
  

